<!DOCTYPE html>
<html lang="en">
  <link rel="stylesheet" type="text/css" href="../css/style.css">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Combined View</title>
      <h3>웹 크롤링 및 분석</h3>
      <hr />
      <img src="../images/감성분석결과.png" alt="">
      <section class="flex-container">
        <div class="flex-container">
          <strong>개요</strong>
          <ul>
          <li>전기차 시장 성장에 따른 변화 파악 필요</li>
          <li>웹크롤링 통한 관련 기사 데이터 수집 및 분석 진행</li>
          <li>*20~23년에는 "보조금", "배터리" 등이 매년 고빈도</li>
          <li>**2022년 이후 긍정 기사가 부정 기사보다 적어짐</li>
          <li>캐즘 현상으로 임시적 수요 정체 가능성</li>
          </ul>
  </div>
  <div class="flex-container">
    <strong>업무</strong>
    <ul>
    <li>기간 : 2024.08.12 ~ 2024.08.24 (2주)</li>
    <li>인원 : 동아일보 미디어 프론티어 과정 3인</li>
    <li>협업 : Slack, Notion</li>
    <li>기여도: 33% (2024년 기사데이터 수집 및 핸들)</li>
    </ul>

  </div>

</section>
      <hr>
      <strong>데이터 과정</strong>
      <section class="flex-container">
        <div class="flex-container">
          <ol>
            <li>수집 (웹 크롤링)</li>
            <ul>
              <li>***Selenium → BeautifulSoup</li>
              <li>포털 다음 "전기차" 8월 9일 검색시 기사 제목 및 언론사 웹크롤링</li>
              <li>2020년~2024년 (연도별 1000개, 총 5000개, 정확도순, CSV)</li>
            </ul>
          </ol>
        </div>
        <div class="flex-container">
          
          <ol start="2">
            <li>전처리 (자연어 분석, 유저 단어 구축)</li>
         
            <ul>
              <li>KoNLPy → Kiwipiepy</li>
              <li>최신성 ↑  간편성 ↑</li>
              <li>최소 소거 : 기사 콘텐츠 (표준성, 통일성 ↑) </li>
              <li>분리: "전기차" ↛  "전기" "차", 공란 처리</li>
            </ul>
          </ul>
        </ul>
      </ol>
      </div>
      
  </section>
  <section class="flex-container">
    <div class="flex-container">
      <ol start="3">
        <li>분석 및 시각화</li>
        <ul>
            <li>Pandas를 활용한 연관, 고빈도 단어분석</li>
            <li>형태소 분석 - 말뭉치 생성 - 문서 단어 행렬화</li>
            <li>고빈도,저빈도 단어 상위 10개 출력 </li>
            <li>Matplotlib, Wordcloud를 활용한 막대 그래프, 워드 클라우드</li>
          </ul>


      </ul>
    </ol>
    </div>
    <div class="flex-container">
<ol start="4">
  <li>감성분석</li>
  <ul>
            <li>OpenAI API를 활용한 긍정/부정/중립 분석</li>

        <li>모델: gpt-4o-mini</li>
        <li>역할: 감성분석 챗봇, 긍정, 부정 중립 표시 </li>
        <li>판별: <a href="../images/감성분석ML.png">ML</a> → <a href="../images/감성분석AI.png">LLM</a> </li>

    </ul>
  </ol>
    </div>

  </section>
  <hr />

      <strong>각주</strong> <br>
      *
      <!-- <img src="../images/워드클라우드.png" alt=""> -->
      24년은 "현대차”와 "기아"차가 고빈도였다. 이는 포털사 기사 정렬 이슈로 보인다. <a href="../images/워드클라우드.png">워드클라우드.jpg</a>
<br>**  긍정-부정:  +8.82% → -8.06% (21년 → 22년)
<br>*** 일반적으로 Selenium의 경우 동적 웹 크롤링, BeautifulSoup의 경우 정적 웹 크롤링에 사용된다.
<br>****
<summary>
<details>
<pre>
#감성분석 코드 
import os
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

def openai(api_key, user_input):
client = OpenAI(api_key=api_key)

completion = client.chat.completions.create(
model='gpt-4o-mini',
messages=[
{'role': 'system', 'content': '너는 전기차 뉴스 제목의 감성분석을 하는 챗봇이야, 제목이 긍정, 부정, 중립인지만 표시해줘.'},
{'role': 'user', 'content': user_input},

]

return completion.choices[ø].message.content

#웹크롤링

import requests
import pandas as pd
import time
from pprint import pprint
from bs4 import BeautifulSoup

results = []
# 헤드라인 10개씩 100page 를 수집 (도합 연 1000개의 헤드라인 수집)
for page in range(1, 101):

URL = f'https://search.daum.net/search?w=news&nil_search=btn&DA=STC&enc=utf8&cluster=y&cluster_page=1&q=%EC%A0%84%EA%B8%B0%EC%B0%A8&sd=2023010

res = requests.get (URL)
time.sleep(0.5)
soup = BeautifulSoup(res.text, 'html.parser')

import csv

presses = soup.select('div.area_tit > div > a > strong > span')
headlines = soup.select('div.item-bundle-mid > div.item-title > strong > a')

# 데이터를 파일에 저장 (예시로 csv 파일 모두 생성)
for press, headline in zip(presses, headlines) :
results.append({"press": press.text.strip(), "headline": headline.text. strip()})

# 데이터프레임으로 변환
news_data = pd.DataFrame(results)

# csV 파일로 저장
news_data.to_csv('2023.csv', index=False)

$전처리
sens = df [ 'Headline' ]

sens = sens. str. replace(pat = '[^-§'A-Za-z0-9 . ] | &. +; ', repl = ' ', regex = True)

i = 0

sens.iloc[1]

' 전기차알고타면기똥차 '

sens = sens. str. replace(pat = ')| ', repl = ' ', regex = True)
sens = sens. str. replace(pat = 'ih ', repl = ' ', regex = True)

kiwistop = Stopwords()

kiwi = Kiwi()

kiwi. tokenize(text = sens[i], stopwords = kiwistop)

[ Token (form=' ChAl', tag='MAG', start=0, len=2),
Token (form='2L', tag='NNG', start=4, len=3),
Token (form='2Lh', tag='VV', start=7, len=2),
Token(form='§2|', tag='VV', start=10, len=2),
Token(form=' 7)} ', tag=' EF', start=11, len=2) ]

tokens = kiwi. tokenize(text = sens[i], stopwords = kiwistop)

pos1, pos2 = ['VV', 'VA' ], [ 'NNG', 'NNP' ]

tokens = [token. form + 'Ch' if token. tag in pos1 else token. form
for token in tokens if token. tag in pos1 + pos2]
</pre>
</details>
</summary>
</section> 


<!-- <h3>후석 일지</h3>
<hr>
<strong>주제 접근</strong>

<ul>
<li>과제의 제목은 &quot;키워드를 선정하여 뉴스 데이터를 수집하고, 텍스트 전처리 및 시각화 작업을 실행한 결과를 정리하여 발표&quot;하는 것이었다. 우선 새로 구성된 팀원들과 함께 어떤 키워드와 주제를 선정하면 좋을지 고민했다. 사실 나는 &quot;정치적 올바름&quot;이라는 사회 현상을 언론의 뉴스에서 분석하면 어떨까 생각했다. 정치적 올바름(political correctness, PC)은 위키백과에 따르면 &quot;말의 표현이나 용어의 사용에서, 인종·민족·언어·종교·성차별 등의 편견이 포함되지 않도록 하자는 주장을 나타낼 때 쓰는 말&quot;이라고 정의되어 있다. 이러한 언어 운동이 최근 시민과 언론의 언어 습관에도 반영되었고, 이런 영향을 시간의 흐름에 따라 뉴스에서 자주 사용되는 단어와 사용되지 않는 단어로 분석하면 좋을 것 같았다.</li>
<br><li>예를 들어, &quot;반려동물&quot;이라는 단어가 떠올랐다. 내가 어릴 때만 해도 반려동물보다는 애완동물이라는 말을 많이 사용했는데, 어느 순간부터 애완동물이라는 단어 사용이 사라졌다. 이는 시민들 사이에서 동물을 &quot;완구&quot;처럼 놀이의 대상이 아니라 동반자처럼 여기기 위해 &quot;반려&quot;라는 단어를 사용해야 한다는 여론이 일었기 때문이다. 이와 비슷한 맥락으로 사용이 지양된 단어로는 &quot;눈먼 돈&quot;, &quot;애꾸눈&quot; 등이 있었다. 이러한 언어 운동과 관련된 단어들을 키워드로 설정하고, 연도별로 뉴스 데이터를 분석하면 재미있을 것 같았다. 그러나 대중 설득의 측면에서 무시할 수 없는 문제도 하나 있었다. 주제 자체가 타인들에게 발표하기에 정치적으로 부담스러운 아이템이라는 점이다. 결론이 어떻게 나오든지 간에 이러한 주제는 듣는 이로 하여금 은연중에 불편함을 느끼게 할 수 있었다. 이에 어느 정도 사회적인 성격을 가지면서도 호불호 없이 듣는 사람들을 설득할 수 있는 키워드가 필요했다.</li>
<br><li>결과적으로는 "전기차" 키워드를 설정했다. 이는 위에서 필요했던 범사회적 수용성을 충족할 수 있는 키워드라고 판단했다. 정치적 아젠다 측면에서 전기차는 기후위기의 주범으로 꼽히는 내연기관을 대체한다는 점에서 진보적인 산업 아이템으로 볼 수 있다. 하지만 자동차라는 기존 산업은 우리나라 수출액 2위를 차지하는 거대한 자본주의 사업이기도 하다. 결국 듣는 이들이 정치 성향에 따라 전기차에 대해 무조건 긍정적으로 평가할 수도, 부정적으로 평가할 수도 없는 입체적인 주제라고 판단했다. 또한 전기차라는 선진 테크 프로덕트는 듣는 사람들이 흥미를 돋구고, 화제성 측면에서는 과제 설정일 기준으로 청라 지하 주차장 화재가 크게 이슈가 되었기에 이를 데이터로 분석하는 것도 흥미로울 것 같았다.</li>
</ul>
<strong>가설설정</strong>
<hr>
<ul>
<li>대신 이번 프로젝트에서는 키워드 선정보다 임의 가설 설정에 더 많은 고민을 했다. 청라 화재 사건이나 전기차 보조금 감소와 같은 안 좋은 뉴스들이 뒤숭숭하게 들리면서 가장 먼저 떠올린 러프한 가설은 "전기차에 대한 여론이 갈수록 나빠졌다"였다. 이 가설 자체는 문제가 없어 보였다. 나 역시 사회를 살아가는 소비자의 입장에서 전기차에 대한 반응이 예전보다 부정적이라는 것에 심리적으로 동의했다.
</li>
<br><li>하지만 이를 뉴스 데이터로 검증할 수 있느냐는 또 다른 문제였다. 여기에는 도메인 비즈니스 문제가 있었다. 전기차는 함부로 부정적인 기사를 쓰기 어려운 소재라고 판단했기 때문이다. 
  첫째 이는 미디어의 수익 구조와 연결돼 보였다. 간단히 정리하면, 국내 언론사는 광고 수익으로 운영되며, 국내 최대 광고주는 당연히 4대 대기업이다. 이런 비즈니스 구조의 옳고 그름을 논하고자 하는 것이 아니라, 데이터 소스 측면에서 보면 이 섹터 자체가 언론사에서 전기차에 대한 부정적인 기사를 생성하기 어려운 구조로 보였다. 설사 부정적인 기사가 나오더라도 기사 물량의 차원에서 긍정적인 기사들로 덮일 가능성이 크지 않을까. </li>
<br><li>두 번째로는 논조 문제였다. 앞서 언급한 범사회성은 데이터 측면에서 단점이 될 수 있었다. 우리나라 언론사들은 양당 체제를 중심으로 움직이고 있는데, 이러한 포지셔닝을 고려하면 전기차에 대해 확실한 긍정도 부정도 아닌 신중하고 모호한 논조를 취할 수밖에 없지 않을까. 그러면 "전기차에 대한 여론이 갈수록 나빠졌다"라는 우리의 심리적 가설을 객관적으로 입증하기에 적합하지 않은 키워드가 되는 것이다.</li>
<br><li>그럼에도 불구하고 팀원들과 상의 후 일단 진행하기로 했다. 이 부분은 실무적인 부분인데 일을 하다보면, 특히 프로젝트에서는 완벽한 전략이나 시기를 오리라는 기대는 애초에 안하는게 좋은거 같다.</li>
</ul>
<strong>감성분석 기획</strong>
<hr>
<ul>
<br><li>우리의 메인 목적은 “전기차에 대한 여론”을 파악하는 것이다. 자동화된 뉴스 여론 분석 과정은 크게 두 가지로 나눌 수 있었다: 1) 뉴스를 수집하고 발췌한 후 이를 정리하는 과정과 2) 정리된 자료를 기계에 의해 판별하도록 요청하는 과정이다.</li>
<br><li>1) 뉴스 자료 수집 및 발췌 정리 과정에서는 크롤링 프로그램을 사용했다. 일반적으로 ‘크롤링’은 웹상에 존재하는 자료들을 특정한 방식으로 수집하는 것을 의미한다. 사실 크롤링 행위 자체는 크게 논의할 사항이 아니었고, 어떤 크롤러를 어디서 얼마나 사용할 것인지에 대한 논의가 필요했다.</li>
<br><li>크롤링 방식에는 크게 정적 크롤링과 동적 크롤링 두 가지가 있다고 한다. 정적 크롤링은 구현 시 화면 이동이 거의 없는 방식이다. 컴퓨터가 눈에 띄지 않게 데이터를 대량으로 획하고 수집하는 모습을 보고 정적 크롤링이라 부른 것 같다. 다만, 이 방식은 사용자에게는 편리하지만, 상대방, 즉 웹사이트 서버에는 부담을 줄 수 있다는 단점이 있어보였다. 내 컴퓨터가 효율적으로 데이터를 빠르게 가져오는 만큼, 상대방의 서버에는 과부하가 발생할 수 있기 때문이다.</li>
<br><li>반대로 동적 크롤링은 화면의 이동이 규칙적으로 이루어지는 방식이다. 모르는 사람이 보면 컴퓨터가 해킹된 것처럼 화면이 스스로 이동하면서 데이터를 수집하는 모습을 보고 동적 크롤링이라는 이름을 붙인 것 같다. 정적 크롤링과는 반대의 관계처럼 보였다. 간단한 크롤링에도 몇 시간이 걸릴 수 있는 만큼, 나의 작업 효율은 감소하지만 상대방의 서버에 가해지는 부담과 위험을 줄일 수 있는 것으로 보였다. 결국 프로젝트 제출기한이라는 시간적 제약이 있는 상황에서 우리는 데이터를 빠르게 수집할 수 있는 정적 크롤러를 사용하기로 했다.</li>
<br><li>2) 기계 판별 요청에 대해서는 챗GPT의 API를 사용하기로 했다. 지금 우리에게 필요한 것은 절대적으로 옳은 기준보다는 이번 작업에서만큼은 일관된 기준이었다. 딥러닝 모델을 사용하면 표면적으로는 판별의 기계적 객관성을 가질 수 있다. 하지만 일관성을 담보하기 어려운 측면이 있었다. 훈련 데이터의 문제와 정답을 제공하는 과정에서 사람의 주관이 개입될 수밖에 없고, 이로 인해 이후 판별의 일관성이 유지되지 않을 가능성이 있었다. 이는 평가자의 주관적 시선과 이데올로기 배경지식이 강하게 반영될 수 있는 뉴스 자연어 데이터의 경우 더욱 그렇다고 생각되었다. 예를 들어, 나의 배경 경험 때문에 똑같은 기사 제목을 보더라도 진보 매체의 경우에는 비판적이라고 판별하고, 보수 매체의 경우에는 긍정적이라고 판별할 수 있다. 그래서 그 신뢰성에 대해서 논란이 많은 트랜스포머 모델이지만, 이번 작업에서는 일관되게 기계적 평가를 진행하기 위해 챗GPT의 판별을 사용하기로 했다.</li>
</ul>
<strong>워드클라우드 기획</strong>
<hr>
<ul>
<li>사이드 발표 기획으로는 워드 클라우드를 준비했다. 뉴스 크롤링을 통해 얻은 소중한 자료를 한번더 가공해 잘 활용한다면 발표 내용을 더욱 풍부하게 만들 수 있어보였다. 사이드 발표 접근의 복습에 초점을 두기로 팀원과 합의했다. 수업 시간에 배운 파이썬과 주피터랩의 지식을 활용하기 위해, 우리는 Matplotlib을 이용한 워드 클라우드를 선택했다. 이러한 워드 클라우드를 만들기 위한 핵심은 1) &quot;워드&quot;를 가려내기 위한 형태소 분석과 2) 사용자 사전 단어 구축이라고 판단했다.</li>
<br><li>1) 한국어 형태소 처리는 Kiwipiepy 라이브러리를 이용해 전처리하기로 했다. 다른 후보군으로는 KoNLPy가 있었지만, Kiwi를 선택한 이유는 위에 언급한 학습성 차원에서였다. 정규 수업 시간에 배운 코드를 활용하고, 자연어 처리를 직접 경험해보는 것이 목표였다. 나중에 조사해보니 두 모델 간에 큰 차이는 없었고, 오히려 모호성 해소 측면에서 Kiwi가 더 낫다는 평가도 있어 안심할 수 있었다.</li>
<br><li>2) 사용자 사전 단어 구축에서는 기사(헤드라인, 부제, 언론사)와 댓글에 대한 접근 전략을 달리했다. 기본적으로 언론사 이름,  헤드라인, 부제의 크롤링 내용에 대한 사용자 단어 설정은 최소한의 소거만했다. 공식적인 기사에서는 맞춤법이나 명칭 등이 어느 정도 통일되어 있어 크게 손을 보지 않아도 될것같다 생각했다. 그래서 &quot;전기차&quot;에서 &quot;전기&quot;와 &quot;차&quot;가 큰 의미 없이 데이터를 후처리 방해하는 것만 막기 위해 이 두 단어를 공란으로 대체하기로 했다.</li>
<br><li>반면, 댓글의 경우에는 최대한 추가 및 변형된 사용자 단어 설정을 하고자 했다. 댓글은 일반 유저가 작성하기 때문에 비속어, 오타, 약어 등이 많을 것으로 판단했기 때문이다. 축약형 등을 정규 단어로 바꾸는 사용자 단어 사전을 만들어 이를 보완하고자 했다.</li>
</ul>
<strong>분석결과</strong>
<hr>
<ul>
<br><li>결과적으로 전기차에 대한 여론이 갈수록 나빠지고 있음을 확인할 수 있었다. 2020년부터 2023년까지 챗GPT 4.0 미니 프롬프트를 이용해 기사 제목을 중심으로 감성 분석을 진행한 결과, 기사 개수를 기준으로 긍정 여론은 전반적으로 하향세를 보였고, 부정 여론은 지속적으로 상승했다. 긍정 여론이 앞서던 상황에서 2022년을 기점으로 부정 여론이 더 많아졌고, 2023년에는 이 격차가 더욱 확대되었다. 중립 여론의 경우, 시작 연도와 종료 연도 간의 차이가 4.86%로, 세 가지 여론 중 가장 작았다. 반면, 부정 여론의 차이는 14.93%로 가장 컸다.</li>
<br><li>(* 감성 분석 결과에서 2024년 데이터는 제외했다. 2024년 8월 8일 15시경 다음 기준으로 수집된 “전기차” 크롤링 자료는 지나치게 최근 화제성 위주로 수집된 것으로 보인다. 이는 2024년 워드 클라우드 결과가 이전 연도들과 지나치게 다른 점에서 더욱 드러났다. 원인은 “정확도” 검색 정렬 기준의 문제로 추측된다. 예를 들어, 2024년 8월 8일에 “정확도” 순으로 “전기차”에 대한 검색을 기준 연도 2024년에 하면, 며칠 사이에 최근 화제가 된 청라 화재 사건 뉴스가 1페이지부터 10페이지까지 주로 잡히지만, 기준 연도 2023년으로 하면 그해 전체에 대한 전기차 기사가 1페이지부터 10페이지까지 고르게 잡히기 때문이다. 2020~2023년까지는 한해 전체 데이터가 동일하게 정렬된 것으로 보여 분석을 진행했다.)</li>
</ul>

<ul>
<br><li>기사 워드 클라우드 결과, 전기차와 관련해서 비즈니스 키워드들이 많이 등장했음을 알 수 있었다. 2020년부터 2023년까지 “배터리, 테슬라, 보조금” 등 기업이나 정책과 관련된 키워드들이 꾸준히 메인으로 등장했다. 일부 연관 단어에 대해서는 더 세밀한 도메인 조사가 필요하겠지만, 2020년에는 2030년까지의 비전 선포 때문인지 현대차 정의선 회장의 이름이 등장했다. 우리 데이터에 따르면, 부정적 여론이 긍정적 여론을 처음 앞지른 2022년에는 &#39;둔화&#39;라는 단어가 처음 워드 클라우드에 잡혔다. (마찬가지로 2024년도 워드 클라우드 데이터는 이전 년도들에 비해 자료들이 너무 튀었다. )</li>
<br><li>댓글 워드 클라우드 결과, 청라 전기차 화재 사건 관련 단어들이 많이 등장했다. “벤츠, 화재, 아파트” 등이 눈에 띄었다. 댓글 데이터는 2024년 8월 10일 &#39;정확도&#39;를 기준으로 네이버 뉴스에서 수집되었다. 네이버 또한 &#39;정확도&#39; 순으로 정렬해도 최근 기사에 강한 가중치를 주는 것으로 추측할 수 있었다. 나는 유저, 즉 소비자 여론을 파악하기 위해 댓글 워드 클라우드를 진행했다. 하지만 단어 상으로 “화재, 문제” 등이 등장했다고 해서 바로 소비자 여론이 부정적이라고 해석하기에는 무리가 있어 보였다. 예를 들어, 소비자 댓글에 “벤츠 전기차라면 화재에도 불구하고 타야지”와 같이 부정적인 단어가 포함되었지만, 결국 긍정적인 뉘앙스를 풍기는 경우가 있을 수 있기 때문이다.</li>
</ul>
<strong>피드백 및 보완</strong>
<hr>
<ul>
<br><li>결론적으로, 전체적인 플로우는 괜찮았으나 실제 도메인 영역과 세부 조정 부분에서는 아쉬움이 남는 프로젝트였다. 예를 들어, 크롤러 부분에서 어떤 크롤러를 사용했는지에 대해서는 논의가 됐지만, 어디서 어떻게 사용할지는 그냥 이뤄진 면이 있었다. 우리는 다음 뉴스 1000개를 크롤링했지만, 왜 다음이어야 했을까. 국내 대형 포털사에는 다음과 네이버가 있는데, 데이터 선택의 대표성 측면에서 두 포털사 모두를 선택하거나, 아니면 다음이어야 하는 강한 이유에 대해 논의가 있었더라면 좋았을 것이다. 또한, 왜 1000개였는지도 의문이었다. 적합한 크롤링 데이터 분량을 정확히 알 수는 없지만, 데이터 라벨링 관련 일을 했던 팀원에 따르면 1000개는 일단 적다는 의견이 있었다. 데이터 분석 측면에서 적합한 시간대과 분량이 더 있었을 것 같다.</li>
<br><li>전기차 도메인과 관련해서는 최근 5년치 데이터로 설정한 것이 왜 최신순이어야 했는지 고민해볼 필요가 있었다. 우리의 로직은 &quot;전기차에 대한 여론이 나빠지고 있으며, 이는 우리가 크롤링하고 감성 분석한 데이터에서도 드러난다. 이는 사업 주기적으로 초기 시장에서 중기 시장으로 넘어갈 때 잠시 수요 정체가 일어나는 캐즘 현상으로 보인다&quot;는 것이었다. 그렇다면 테슬라의 전기차 시장 도입 초기인 2010년부터 트래킹하는 것이 더 좋았을 것 같다.</li>
<br><li>전처리 측면에서도 사용자 단어 설정에 대한 컨셉이 필요하다. 예를 들어 댓글 분석에서 80개 가량의 단어 사전을 구축했지만, 단순 비속어 처리와 요약어 처리 외에도 어르신들이 하기 쉬운 실수나 젊은 사람들이 비판하는 단어에 대한 전략을 세워 접근했으면 좋았을 것이다. 사용자 단어 구축 문제는 어렵기보다는 까다롭고, 언어적 경우의 수가 많기 때문에 완벽을 추구하기보다는 구축 컨셉이나 노하우에 대해 상의할 멘토가 필요해 보인다.</li>
<br><li>그럼에도 불구하고, 코딩과 컴퓨팅 기술에 대해 새로운 시각을 가지게 된 계기가 있었다. 코딩과 컴퓨팅의 궁극적인 목적이 인간의 편리성 증대 및 작업 효율화라고 생각해왔지만, 이번에는 활용 윤리와 가이드라인에 대해 고민해보았다. 특히 크롤링 행위에 대해 생각해보았다. 우리는 정적 크롤링을 통해 효율적으로 과제를 진행했지만, 이는 비상업적 학습 용도로 공개된 데이터를 소량 가져오는 것이어서 용납 가능한 방법이었던 것 같다. 만약 상업적이고 지속적이며 대규모적인 크롤링이었다면 상대방이 공격받는다고 느낄 수도 있었을 것이다. 이 경우 법적 및 상업적인 배상과 책임 문제로 이어질수도 있어보였다. 컴퓨팅의 효율성도 좋지만 그 방식에 대한 기준도 앞으로 생각해볼 필요가 있어보였다.</li>
</ul> -->

<style>
  body {
          max-width: 800px; 
  }
</style>

</body>
</html>
