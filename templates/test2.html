<!DOCTYPE html>
<html lang="en">
  <link rel="stylesheet" type="text/css" href="../css/style.css">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Combined View</title>
      <h3>웹 크롤링 및 분석</h3>
      <hr />
      <img src="../images/감성분석결과.png" alt="">
      <section class="flex-container">
        <div class="flex-container">
          <strong>개요</strong>
          <ul>
          <li>전기차 시장 성장에 따른 변화 파악 필요</li>
          <li>웹크롤링 통한 관련 기사 데이터 수집 및 분석 진행</li>
          <li>*20~23년에는 "보조금", "배터리" 등이 매년 고빈도</li>
           →  전기차 산업은 “기간 사업”, “글로벌 산업” <br>
          <li>**2022년 이후 긍정 기사가 부정 기사보다 적음</li>
           → 캐즘 현상으로 임시적 수요 정체 가능성
          </ul>
  </div>
  <div class="flex-container">
    <strong>업무</strong>
    <ul>
    <li>기간 : 2024.08.12 ~ 2024.08.24 (2주)</li>
    <li>인원 : 동아일보 미디어 데이터 과정 3인</li>
    <li>협업 : Slack, Notion</li>
    <li>기여도: 33% (2024년 기사데이터 수집 및 핸들)</li>
    </ul>

  </div>

</section>
      <hr>
      <strong>데이터 과정</strong>
      <section class="flex-container">
        <div class="flex-container">
          <ol>
            <li>수집 (웹 크롤링)</li>
            <ul>
              <li>***Selenium → BeautifulSoup</li>
              <li>포털 다음 "전기차" 8월 9일 검색시 기사 제목 및 언론사 웹크롤링</li>
              <li>2020년~2024년 (연도별 1000개, 총 5000개, 정확도순, CSV)</li>
            </ul>
          </ol>
        </div>
        <div class="flex-container">
          
          <ol start="2">
            <li>전처리 (자연어 분석, 유저 단어 구축)</li>
         
            <ul>
              <li>KoNLPy → Kiwipiepy</li>
              <li>최신성 ↑  간편성 ↑</li>
              <li>최소 소거 : 기사 콘텐츠 (표준성, 통일성 ↑) </li>
              <li>분리: "전기차" ↛  "전기" "차", 공란 처리</li>
            </ul>
          </ul>
        </ul>
      </ol>
      </div>
      
  </section>
  <section class="flex-container">
    <div class="flex-container">
      <ol start="3">
        <li>분석 및 시각화</li>
        <ul>
            <li>Pandas를 활용한 연관, 고빈도 단어분석</li>
            <li>형태소 분석 - 말뭉치 생성 - 문서 단어 행렬화</li>
            <li>고빈도,저빈도 단어 상위 10개 출력 </li>
            <li>Matplotlib, Wordcloud를 활용한 막대 그래프, 워드 클라우드</li>
          </ul>


      </ul>
    </ol>
    </div>
    <div class="flex-container">
<ol start="4">
  <li>감성분석</li>
  <ul>
            <li>OpenAI API를 활용한 긍정/부정/중립 분석</li>

        <li>모델: gpt-4o-mini</li>
        <li>역할: 감성분석 챗봇, 긍정, 부정 중립 표시 </li>
        <li>판별: <a href="../images/감성분석ML.png">ML</a> → <a href="../images/감성분석AI.png">LLM</a> </li>

    </ul>
  </ol>
    </div>

  </section>
  <hr />

      <!-- <strong>각주</strong> <br> -->
      *
      <!-- <img src="../images/워드클라우드.png" alt=""> -->
      24년은 "현대차”와 "기아"차가 고빈도였다. 이는 포털사 기사 정렬 이슈로 보인다. <a href="../images/워드클라우드.png">워드클라우드.jpg</a>
<br>**  긍정-부정:  +8.82% → -8.06% (21년 → 22년)
<br>*** 일반적으로 Selenium의 경우 동적 웹 크롤링, BeautifulSoup의 경우 정적 웹 크롤링에 사용된다.
<!-- <br>**** -->
<!-- <summary>
<details>
<pre>
#감성분석 코드 
import os
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

def openai(api_key, user_input):
client = OpenAI(api_key=api_key)

completion = client.chat.completions.create(
model='gpt-4o-mini',
messages=[
{'role': 'system', 'content': '너는 전기차 뉴스 제목의 감성분석을 하는 챗봇이야, 제목이 긍정, 부정, 중립인지만 표시해줘.'},
{'role': 'user', 'content': user_input},

]

return completion.choices[ø].message.content

#웹크롤링

import requests
import pandas as pd
import time
from pprint import pprint
from bs4 import BeautifulSoup

results = []
# 헤드라인 10개씩 100page 를 수집 (도합 연 1000개의 헤드라인 수집)
for page in range(1, 101):

URL = f'https://search.daum.net/search?w=news&nil_search=btn&DA=STC&enc=utf8&cluster=y&cluster_page=1&q=%EC%A0%84%EA%B8%B0%EC%B0%A8&sd=2023010

res = requests.get (URL)
time.sleep(0.5)
soup = BeautifulSoup(res.text, 'html.parser')

import csv

presses = soup.select('div.area_tit > div > a > strong > span')
headlines = soup.select('div.item-bundle-mid > div.item-title > strong > a')

# 데이터를 파일에 저장 (예시로 csv 파일 모두 생성)
for press, headline in zip(presses, headlines) :
results.append({"press": press.text.strip(), "headline": headline.text. strip()})

# 데이터프레임으로 변환
news_data = pd.DataFrame(results)

# csV 파일로 저장
news_data.to_csv('2023.csv', index=False)

$전처리
sens = df [ 'Headline' ]

sens = sens. str. replace(pat = '[^-§'A-Za-z0-9 . ] | &. +; ', repl = ' ', regex = True)

i = 0

sens.iloc[1]

' 전기차알고타면기똥차 '

sens = sens. str. replace(pat = ')| ', repl = ' ', regex = True)
sens = sens. str. replace(pat = 'ih ', repl = ' ', regex = True)

kiwistop = Stopwords()

kiwi = Kiwi()

kiwi. tokenize(text = sens[i], stopwords = kiwistop)

[ Token (form=' ChAl', tag='MAG', start=0, len=2),
Token (form='2L', tag='NNG', start=4, len=3),
Token (form='2Lh', tag='VV', start=7, len=2),
Token(form='§2|', tag='VV', start=10, len=2),
Token(form=' 7)} ', tag=' EF', start=11, len=2) ]

tokens = kiwi. tokenize(text = sens[i], stopwords = kiwistop)

pos1, pos2 = ['VV', 'VA' ], [ 'NNG', 'NNP' ]

tokens = [token. form + 'Ch' if token. tag in pos1 else token. form
for token in tokens if token. tag in pos1 + pos2]
</pre> -->
</details>
</summary>
</section> 




<style>
  body {
          max-width: 800px; 
  }
</style>

</body>
</html>
